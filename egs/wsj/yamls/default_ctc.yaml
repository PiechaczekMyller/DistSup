Datasets:
  train:
    class_name: PaddedDatasetLoader
    batch_size: 16
    dataset:
      class_name: egs.wsj.data.WSJData
      split: train_si284
    shuffle: true
    num_workers: 6
    pin_memory: true
  dev:
    class_name: PaddedDatasetLoader
    batch_size: 32
    dataset:
      class_name: egs.wsj.data.WSJData
      split: test_dev93
    num_workers: 4
    pin_memory: true
  test:
    class_name: PaddedDatasetLoader
    batch_size: 32
    dataset:
      class_name: egs.wsj.data.WSJData
      split: test_eval92
    num_workers: 4
    pin_memory: true


Model:
  class_name: distsup.models.sequential.SimpleCTCModel
  num_classes: 64
  adv_size: 283
  encoder:
    class_name: distsup.modules.encoders.DeepSpeech2
    num_input_channels: 3
    input_height: 81
    conv_strides: [[1,2], [3,1]]
    conv_kernel_sizes: [[7, 7], [7, 7]]
    rnn_hidden_size: 320
    rnn_nb_layers: 4
    rnn_normalization: none

Trainer:
  num_epochs : 85
  output_frequency: 1
  checkpointer:
    every_n_hours: 4
  optimizer_name : Adam
  learning_rate : 0.001
  learning_rate_scheduler:
    class_name: torch.optim.lr_scheduler.MultiStepLR
    gamma: 0.5
    milestones: [32, 37, 42, 47, 52, 57, 62, 67, 72]
  gradient_clipping:
    clip_norm: 1000
    skip_step_norm: 10000
  weight_noise: 0.15
  weight_noise_start_iteration: 20000
  polyak_decay:
  - 0.9998
