Datasets:
  dev:
    batch_size: 32
    class_name: PaddedDatasetLoader
    dataset:
      alignment_root: data/scribblelens.paths.1.4b.zip
      class_name: egs.scribblelens.data.ScribbleLensDataset
      split: test
      transcript_mode: 5
      vocabulary: egs/scribblelens/tasman.alphabet.plus.space.mode5.json
    rename_dict:
      image: features
  train:
    batch_size: 32
    class_name: FixedDatasetLoader
    dataset:
      chunk_len: 128
      class_name: distsup.data.ChunkedDataset
      dataset:
        alignment_root: data/scribblelens.paths.1.4b.zip
        class_name: egs.scribblelens.data.ScribbleLensDataset
        split: train
        transcript_mode: 5
        vocabulary: egs/scribblelens/tasman.alphabet.plus.space.mode5.json
      drop_fields:
      - text
      - alignment_rle
      - page_side
      - page
      training: true
      oversample: 4
      varlen_fields:
      - image
      - alignment
    num_workers: 4
    pin_memory: true
    rename_dict:
      image: features
    shuffle: true
Model:
  class_name: distsup.models.representation_learners.RepresentationLearner
  image_height: 32
  reconstructor:
    class_name: distsup.modules.reconstructors.ColumnGatedPixelCNN
    hid_channels: 64
  bottleneck:
    class_name: distsup.modules.bottlenecks.GumbelBottleneck
    num_tokens: 4096
    initial_temp: 1.0
    temp_factor_per_epoch: 0.92  # 0.005 in 50 epochs
    min_temp: 0.001
  bottleneck_latent_dim: 128  # Must match hid_size in enc_sup
  encoder:
    class_name: distsup.modules.encoders.DeepSpeech2
    # Convolutional top layers
    conv_strides: [[1, 2], [2, 2], [1, 1], [1, 2], [1, 1], [1, 2], [1, 1]] # Best setup so far v3, 2x longer path than ORG
    conv_kernel_sizes: [[3, 3], [5, 5], [3, 3], [3, 3], [3, 3], [3, 3], [3, 2]]   # Best setup so far v3

    conv_num_features: [64, 128, 256, 256, 512, 512, 512]
    conv_nonlinearity: leakyrelu # hardtanh, leakyrelu, relu

    # (B)LSTM + CTC
    rnn_hidden_size: 32
    rnn_projection_size: 32
    rnn_normalization: batch_norm # Choices: batch_norm, none, or instance_norm. Active BN is very effective here.
    rnn_subsample: [1, 1]
    rnn_nb_layers: 2
    rnn_bidirectional: True
    rnn_bias: True
    rnn_residual: True
    rnn_dropout: 0      # If larger than 0 then put a drop-out layer in between LSTM
  probes: !include ${DISTSUP_DIR}/yamls/_default_probes.yaml

Trainer:
  checkpointer:
    every_n_hours: 1
  gradient_clipping:
    clip_norm: 10.0
  learning_rate: 2.e-3
  learning_rate_scheduler:
    class_name: torch.optim.lr_scheduler.ReduceLROnPlateau
    factor: 0.8
    patience: 3  # XXX
    min_lr: 2.e-5
  output_frequency: 50
  num_epochs: 50
  optimizer_name: Adam
  optimizer_kwargs:
    # Default beta2: 0.9999. Aim roughly for (1.-beta2)^nSamples ~~ 0.3 iff you want momentum at end of train dataset
    betas: [0.9, 0.999]
  weight_noise: 0.0
  weight_noise_start_iteration: 10000
